{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfMuITSUiJYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c267d231-6963-4712-944a-fd27f2638193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "ngrok - tunnel local ports to public URLs and inspect traffic\n",
            "\n",
            "USAGE:\n",
            "  ngrok [command] [flags]\n",
            "\n",
            "AUTHOR:\n",
            "  ngrok - <support@ngrok.com>\n",
            "\n",
            "COMMANDS: \n",
            "  config          update or migrate ngrok's configuration file\n",
            "  http            start an HTTP tunnel\n",
            "  tcp             start a TCP tunnel\n",
            "  tunnel          start a tunnel for use with a tunnel-group backend\n",
            "\n",
            "EXAMPLES: \n",
            "  ngrok http 80                                                 # secure public URL for port 80 web server\n",
            "  ngrok http --domain baz.ngrok.dev 8080                        # port 8080 available at baz.ngrok.dev\n",
            "  ngrok tcp 22                                                  # tunnel arbitrary TCP traffic to port 22\n",
            "  ngrok http 80 --oauth=google --oauth-allow-email=foo@foo.com  # secure your app with oauth\n",
            "\n",
            "Paid Features: \n",
            "  ngrok http 80 --domain mydomain.com                           # run ngrok with your own custom domain\n",
            "  ngrok http 80 --allow-cidr 2600:8c00::a03c:91ee:fe69:9695/32  # run ngrok with IP policy restrictions\n",
            "  Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Flags:\n",
            "  -h, --help      help for ngrok\n",
            "\n",
            "Use \"ngrok [command] --help\" for more information about a command.\n",
            "\n",
            "PYNGROK VERSION:\n",
            "   7.1.6\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.3-py3-none-any.whl (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.67-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!pip install pyngrok\n",
        "\n",
        "!ngrok authtoken 2ZaukEX2dpEdTGQatDve7dPpmGL_3u8Eur4MtR4aXCYparH96\n",
        "\n",
        "!ngrok\n",
        "\n",
        "!pip install langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install PyPDF2\n",
        "!pip install langchain sentence_transformers\n",
        "!pip install huggingface_hub\n",
        "!pip install transformers\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile htmlTemplates.py\n",
        "\n",
        "\n",
        "user_template = '''\n",
        "    <div class=\"chat-message user\" style=\"padding: 0.5rem; border-radius: 2rem; margin-bottom: 0.25rem; display: flex;\">\n",
        "        <div class=\"avatar\">\n",
        "            <img src=\"https://th.bing.com/th/id/OIP.7G6XwS4BzQWHQl-VoyvCFgHaHa?rs=1&pid=ImgDetMain\" style=\"width: 60px; height: 60px;\">\n",
        "        </div>\n",
        "        <div class=\"message\" style=\"background-color: #FFFFFF;\">\n",
        "            {message}\n",
        "        </div>\n",
        "    </div>\n",
        "    '''\n",
        "\n",
        "bot_template = '''\n",
        "    <div class=\"chat-message bot\" style=\"padding: 1rem; border-radius: 2rem; margin-bottom: 0.25rem; display: flex;\">\n",
        "        <div class=\"avatar\">\n",
        "            <img src=\"https://th.bing.com/th/id/OIP.j21DfVny1Hush30Nvo-oGAHaHa?rs=1&pid=ImgDetMain\" style=\"width: 60px; height: 60px;\">\n",
        "        </div>\n",
        "        <div class=\"message\" style=\"background-color: #FFFFFF;\">\n",
        "            {message}\n",
        "        </div>\n",
        "    </div>\n",
        "    '''"
      ],
      "metadata": {
        "id": "WszQAbw4rJqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#b0c4de\n",
        "#ADD8E6"
      ],
      "metadata": {
        "id": "8gYaOq2f4dRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import langchain\n",
        "import PyPDF2\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import os\n",
        "from langchain.llms import HuggingFaceHub, OpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from htmlTemplates import bot_template, user_template\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "\n",
        "\n",
        "\n",
        "def get_raw_text(pdf_docs):\n",
        "    text = ''\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf)\n",
        "        for pages in pdf_reader.pages:\n",
        "            text += pages.extract_text()\n",
        "    st.write(\"Raw text extracted\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splliter = CharacterTextSplitter(\n",
        "        separator='\\n',\n",
        "        chunk_size=800,\n",
        "        chunk_overlap=20,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splliter.split_text(text)\n",
        "    st.write(\"Chunks splited\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_vectorestore(text_chunks):\n",
        "    embeddings = HuggingFaceEmbeddings()\n",
        "    vectorestore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    st.write(\"Stored to Vectorestore\")\n",
        "    return vectorestore\n",
        "\n",
        "\n",
        "def get_conversation(vectorstore):\n",
        "\n",
        "\n",
        "    #repo_id = \"tiiuae/falcon-7b-instruct\"\n",
        "    repo_id = \"google/flan-t5-xxl\"\n",
        "\n",
        "    llm = HuggingFaceHub(\n",
        "        repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 500}\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        memory=memory,\n",
        "        retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "\n",
        "def handle_user_input(user_question):\n",
        "    response = st.session_state.conversation({'question':user_question})\n",
        "    st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "    for i, message in enumerate(st.session_state.chat_history):\n",
        "        if i%2==0:\n",
        "            st.write(user_template.replace(\"{message}\", message.content), unsafe_allow_html=True)\n",
        "\n",
        "        else:\n",
        "            st.write(bot_template.replace(\"{message}\", message.content), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "st.set_page_config(page_title='Chat with your PDFs', page_icon=\":books:\")\n",
        "\n",
        "if 'conversation' not in st.session_state:\n",
        "    st.session_state.conversation=None\n",
        "\n",
        "\n",
        "st.header('Chat with your PDFs :books:')\n",
        "user_question = st.text_input('Enter your query here')\n",
        "\n",
        "if user_question:\n",
        "    handle_user_input(user_question)\n",
        "\n",
        "with st.sidebar:\n",
        "    st.subheader('Your Documents')\n",
        "    pdf_docs = st.file_uploader('Upload your files here and press \"Process\"', accept_multiple_files=True)\n",
        "    if st.button('Process'):\n",
        "        with st.spinner('Processing'):\n",
        "          raw_text = get_raw_text(pdf_docs)\n",
        "\n",
        "          #breaking the text into text chunks\n",
        "          text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "          #converting the chunks into embeddings and storing them in vector database\n",
        "          vectorstore = get_vectorestore(text_chunks)\n",
        "\n",
        "          #creating conversational chatbot\n",
        "          st.session_state.conversation = get_conversation(vectorstore)\n",
        "          st.write(\"Process Completed\")"
      ],
      "metadata": {
        "id": "Nz2M8B4okA8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "oDTHGtVHnEsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pgrep streamlit"
      ],
      "metadata": {
        "id": "yjPFpeT8ttQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "public_url"
      ],
      "metadata": {
        "id": "XUJdeYlAniFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6V5lZeB0pk_1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}